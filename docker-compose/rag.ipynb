{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f5e65c6",
   "metadata": {},
   "source": [
    "# Utilisation du LLM préinstallé dans Ollama, de l'Embedding, d'une base FAISS et suivi dans MLflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b931abf",
   "metadata": {},
   "source": [
    "## Lien avec MLFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ef9f80d-f6fe-4c2c-ab83-fed9eeb4451f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "mlflow.set_tracking_uri(\"http://mlflow-serveur:8080/\")\n",
    "mlflow.set_experiment(\"rag\")\n",
    "mlflow.langchain.autolog()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfeed3b5",
   "metadata": {},
   "source": [
    "## Lecture du texte en entrée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a414c56-a86a-4709-b9df-cb4815d699d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(\"data/paul_graham_essay.txt\")\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536fd810",
   "metadata": {},
   "source": [
    "## Découpage du texte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4256f56-2b34-4b7b-9471-874f83de27fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split into 347 chunks\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "# Split into chunks \n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "print(f\"Split into {len(chunks)} chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481c27cc",
   "metadata": {},
   "source": [
    "## Lien avec L'embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb13bb8a-e855-4074-b98d-bceb3ffa5d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.infinity import InfinityEmbeddings\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "embeddings = InfinityEmbeddings(model=\"intfloat/multilingual-e5-base\", infinity_api_url=\"http://embedding:7997\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16138021",
   "metadata": {},
   "source": [
    "## Transformation des bouts de texte en vecteurs (pour en déduire une direction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e52bf5c6-44fd-45d8-a22d-d5e6e8f56dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "vecotr_index = FAISS.from_documents(chunks, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15884701",
   "metadata": {},
   "source": [
    "## Lien avec le LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3d6fabc-52ea-4c38-94b4-08942cf5be5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.llms import Ollama\n",
    "\n",
    "model = Ollama(verbose=True,\n",
    "               base_url=\"http://ollama:11434\",\n",
    "               model=\"llama3\",\n",
    "               )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38248be4",
   "metadata": {},
   "source": [
    "## Fabrication d'une chaine Question / Réponse (QA) + RAG avec les éléments définis au dessus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5be8ccc6-3831-4e90-b9eb-818673e21beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "from langchain.chains import RetrievalQA\n",
    "chain = RetrievalQA.from_chain_type(llm=model,\n",
    "                                             retriever=vecotr_index.as_retriever(),\n",
    "                                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12d0ae82-d972-4d48-98ef-584e382ea49d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RetrievalQA(combine_documents_chain=StuffDocumentsChain(llm_chain=LLMChain(prompt=PromptTemplate(input_variables=['context', 'question'], template=\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\\n{context}\\n\\nQuestion: {question}\\nHelpful Answer:\"), llm=Ollama(verbose=True, base_url='http://ollama:11434', model='llama3')), document_variable_name='context'), retriever=VectorStoreRetriever(tags=['FAISS', 'InfinityEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x7f57e122e5d0>))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46ce058",
   "metadata": {},
   "source": [
    "## Test avec une question, puis remontée dans MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1dd874-a0f4-46f4-a658-8fa7a022c854",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What were the two main things the author worked on before college?\"\n",
    "\n",
    "print(chain.invoke({\"query\": query}, return_only_outputs=True))\n",
    "mlflow.models.set_model(model=chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8feccd",
   "metadata": {},
   "source": [
    "# Personnalisation de l'instruction générative (Prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb2acbb-5297-44e2-bac7-b6143481b51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = '''\n",
    "Use the following pieces of context to answer the question at the end.\n",
    "If you don't know the answer, just say that you don't know.\n",
    "Don't try to make up an answer.\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Answer:\n",
    "'''\n",
    "prompt = PromptTemplate(\n",
    "    template=template, \n",
    "    input_variables=[\n",
    "        'context', \n",
    "        'question',\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Initialise RetrievalQA Chain\n",
    "chain = RetrievalQA.from_chain_type(\n",
    "    model,\n",
    "    retriever=vecotr_index.as_retriever(),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": prompt}\n",
    ")\n",
    "\n",
    "response = chain({\"query\": query})\n",
    "\n",
    "print(response) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
