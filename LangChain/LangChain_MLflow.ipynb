{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f5e65c6",
   "metadata": {},
   "source": [
    "# Utilisation d'un LLM et suivi dans MLflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b9e7d7",
   "metadata": {},
   "source": [
    "## Téléchargement d'un fichier gguf pour LlamaCpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e0f417b-7f3d-4bb6-b4e4-685ed713d043",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import sys\n",
    "\n",
    "# répertoire models\n",
    "if os.path.isdir('models')==False:\n",
    "    os.mkdir('models')\n",
    "\n",
    "# téléchargement du fichier GGUF\n",
    "if os.path.exists('models/llama.gguf')==False:\n",
    "    url = 'https://huggingface.co/QuantFactory/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct.Q4_K_M.gguf?download=true'\n",
    "    r = requests.get(url)\n",
    "    open('models/llama.gguf', 'wb').write(r.content)\n",
    "    print('downloaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238016e3",
   "metadata": {},
   "source": [
    "## Création d'un Experiment dans MLflow pour suivre les différentes exécutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0eaf9005-2a7a-4d1e-88a3-16592e784ddb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='mlflow-artifacts:/995425785044879997', creation_time=1725891041882, experiment_id='995425785044879997', last_update_time=1725891041882, lifecycle_stage='active', name='my-experiment', tags={}>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mlflow\n",
    "mlflow.set_tracking_uri(\"http://mlflow-serveur:8080/\")\n",
    "mlflow.set_experiment(\"my-experiment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c3dd22",
   "metadata": {},
   "source": [
    "## Mise en place du LLM\n",
    "\n",
    "Lors de la première exécution (sans la variable api_key) LlamaCpp est utilisé\n",
    "\n",
    "Puis OpenAI est utilisé (avec la clé api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8dc9ebf-e59d-48c8-b991-02173f27cb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This example demonstrates defining a model directly from code.\n",
    "# This feature allows for defining model logic within a python script, module, or notebook that is stored\n",
    "# directly as serialized code, as opposed to object serialization that would otherwise occur when saving\n",
    "# or logging a model object.\n",
    "# This script defines the model's logic and specifies which class within the file contains the model code.\n",
    "# The companion example to this, chain_as_code_driver.py, is the driver code that performs the  logging and\n",
    "# loading of this model definition.\n",
    "\n",
    "from operator import itemgetter\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_openai import OpenAI\n",
    "from langchain_community.llms import LlamaCpp\n",
    "\n",
    "import mlflow\n",
    "\n",
    "mlflow.langchain.autolog()\n",
    "\n",
    "\n",
    "# Return the string contents of the most recent message from the user\n",
    "def extract_user_query_string(chat_messages_array):\n",
    "    return chat_messages_array[-1][\"content\"]\n",
    "\n",
    "\n",
    "# Return the chat history, which is is everything before the last question\n",
    "def extract_chat_history(chat_messages_array):\n",
    "    return chat_messages_array[:-1]\n",
    "\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"You are a hello world bot. Respond with a reply to the user's question that is fun and interesting to the user.  User's question: {question}\",\n",
    "    input_variables=[\"question\"],\n",
    ")\n",
    "\n",
    "if 'api_key' in vars():\n",
    "    model = OpenAI(temperature=0.95)\n",
    "else:\n",
    "    model = LlamaCpp(\n",
    "      model_path=\"models/llama.gguf\",\n",
    "      n_ctx = 1024,\n",
    "      n_batch= 1024,\n",
    "      max_tokens = 350,\n",
    "      temperature = 0.1,\n",
    "      n_gpu_layers = 20,\n",
    "      verbose = True,\n",
    "    )\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        \"question\": itemgetter(\"messages\") | RunnableLambda(extract_user_query_string),\n",
    "        \"chat_history\": itemgetter(\"messages\") | RunnableLambda(extract_chat_history),\n",
    "    }\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73aa14e1",
   "metadata": {},
   "source": [
    "## Question et remontée dans MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6029aa70-a672-427e-8a9f-898571d9add4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Rag is actually a type of fabric made from scrap cloth pieces. It's been around for centuries and has been used for everything from cleaning to fashion. But did you know that rag rugs were once so valuable that they were passed down as family heirlooms? Now that's one fancy rag!\n"
     ]
    }
   ],
   "source": [
    "question = {\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"what is rag?\",\n",
    "        },\n",
    "    ]\n",
    "}\n",
    "print(chain.invoke(question))\n",
    "\n",
    "# IMPORTANT: The model code needs to call `mlflow.models.set_model()` to set the model,\n",
    "# which will be loaded back using `mlflow.langchain.load_model` for inference.\n",
    "mlflow.models.set_model(model=chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12629ef8-856c-4a74-92dc-a9450b1b1680",
   "metadata": {},
   "source": [
    "## Saisie de la clé pour OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b35be643-2ffd-4694-9a3a-fe00e970c9f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ········\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8bcd4e12-0039-4660-899b-10cde75f0171",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "api_key = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd047776-5ff5-4291-a98b-1504d353ec30",
   "metadata": {},
   "source": [
    "il est maintenant possible de relancer la mise en place du LLM puis la question"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
