{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f5e65c6",
   "metadata": {},
   "source": [
    "# Utilisation du LLM préinstallé dans Ollama et suivi dans MLflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238016e3",
   "metadata": {},
   "source": [
    "## Création d'un Experiment dans MLflow pour suivre les différentes exécutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0eaf9005-2a7a-4d1e-88a3-16592e784ddb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='mlflow-artifacts:/995425785044879997', creation_time=1725891041882, experiment_id='995425785044879997', last_update_time=1725891041882, lifecycle_stage='active', name='my-experiment', tags={}>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mlflow\n",
    "mlflow.set_tracking_uri(\"http://mlflow-serveur:8080/\")\n",
    "mlflow.set_experiment(\"ollama\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c3dd22",
   "metadata": {},
   "source": [
    "## Mise en place du LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8dc9ebf-e59d-48c8-b991-02173f27cb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This example demonstrates defining a model directly from code.\n",
    "# This feature allows for defining model logic within a python script, module, or notebook that is stored\n",
    "# directly as serialized code, as opposed to object serialization that would otherwise occur when saving\n",
    "# or logging a model object.\n",
    "# This script defines the model's logic and specifies which class within the file contains the model code.\n",
    "# The companion example to this, chain_as_code_driver.py, is the driver code that performs the  logging and\n",
    "# loading of this model definition.\n",
    "\n",
    "from operator import itemgetter\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain.llms import Ollama\n",
    "\n",
    "import mlflow\n",
    "\n",
    "mlflow.langchain.autolog()\n",
    "\n",
    "\n",
    "# Return the string contents of the most recent message from the user\n",
    "def extract_user_query_string(chat_messages_array):\n",
    "    return chat_messages_array[-1][\"content\"]\n",
    "\n",
    "\n",
    "# Return the chat history, which is is everything before the last question\n",
    "def extract_chat_history(chat_messages_array):\n",
    "    return chat_messages_array[:-1]\n",
    "\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"You are a hello world bot. Respond with a reply to the user's question that is fun and interesting to the user.  User's question: {question}\",\n",
    "    input_variables=[\"question\"],\n",
    ")\n",
    "\n",
    "model = Ollama(verbose=True,\n",
    "               base_url=\"http://ollama:11434\",\n",
    "               model=\"llama3\",\n",
    "               )\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        \"question\": itemgetter(\"messages\") | RunnableLambda(extract_user_query_string),\n",
    "        \"chat_history\": itemgetter(\"messages\") | RunnableLambda(extract_chat_history),\n",
    "    }\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73aa14e1",
   "metadata": {},
   "source": [
    "## Question et remontée dans MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6029aa70-a672-427e-8a9f-898571d9add4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Rag is actually a type of fabric made from scrap cloth pieces. It's been around for centuries and has been used for everything from cleaning to fashion. But did you know that rag rugs were once so valuable that they were passed down as family heirlooms? Now that's one fancy rag!\n"
     ]
    }
   ],
   "source": [
    "question = {\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"what is rag?\",\n",
    "        },\n",
    "    ]\n",
    "}\n",
    "print(chain.invoke(question))\n",
    "\n",
    "# IMPORTANT: The model code needs to call `mlflow.models.set_model()` to set the model,\n",
    "# which will be loaded back using `mlflow.langchain.load_model` for inference.\n",
    "mlflow.models.set_model(model=chain)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
